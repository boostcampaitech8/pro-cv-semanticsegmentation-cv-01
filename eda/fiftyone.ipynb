{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a335a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2, json, os, glob, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = \"/data/ephemeral/home/data\"\n",
    "TRAIN_IMG_ROOT = \"/data/ephemeral/home/data/train/DCM\"\n",
    "TRAIN_LBL_ROOT = \"/data/ephemeral/home/data/train/outputs_json\"\n",
    "TEST_IMG_ROOT = \"/data/ephemeral/home/data/test/DCM\"\n",
    "META_PATH = \"/data/ephemeral/home/data/meta_data.xlsx\"\n",
    "\n",
    "DATASET_NAME = \"Hand Bone Image Segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4f0b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded: 550 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ë‚˜ì´</th>\n",
       "      <th>ì„±ë³„</th>\n",
       "      <th>ì²´ì¤‘(ëª¸ë¬´ê²Œ)</th>\n",
       "      <th>í‚¤(ì‹ ì¥)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>ì—¬</td>\n",
       "      <td>63.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>ë‚¨</td>\n",
       "      <td>70.0</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>ì—¬</td>\n",
       "      <td>48.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>ì—¬</td>\n",
       "      <td>49.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>ì—¬</td>\n",
       "      <td>50.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  ë‚˜ì´ ì„±ë³„  ì²´ì¤‘(ëª¸ë¬´ê²Œ)  í‚¤(ì‹ ì¥)\n",
       "0   1  30  ì—¬     63.0  165.0\n",
       "1   2  21  ë‚¨     70.0  172.0\n",
       "2   3  21  ì—¬     48.0  164.0\n",
       "3   4  30  ì—¬     49.0  158.0\n",
       "4   5  33  ì—¬     50.0  160.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_cleaned_meta(path) :\n",
    "    df = pd.read_excel(path)\n",
    "    # 1. ë¶ˆí•„ìš”í•œ 'Unnamed' ì»¬ëŸ¼ ì œê±°\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # 2. ì„±ë³„: íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    if 'ì„±ë³„' in df.columns:\n",
    "        df['ì„±ë³„'] = df['ì„±ë³„'].astype(str).str.extract(r'([ê°€-í£]+)')[0].str.strip()\n",
    "    \n",
    "    # 3. ID: ì •ìˆ˜í˜•(int) ë³€í™˜\n",
    "    if 'ID' in df.columns:\n",
    "        df['ID'] = pd.to_numeric(df['ID'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # 4. í‚¤/ëª¸ë¬´ê²Œ: ì‹¤ìˆ˜í˜•(float) ë³€í™˜ (ì†Œìˆ˜ì  ìœ ì§€)\n",
    "    # ìˆ«ìê°€ ì•„ë‹Œ ê°’ì´ ìˆìœ¼ë©´ NaN(ê²°ì¸¡ì¹˜)ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n",
    "    if 'í‚¤(ì‹ ì¥)' in df.columns:\n",
    "        df['í‚¤(ì‹ ì¥)'] = pd.to_numeric(df['í‚¤(ì‹ ì¥)'], errors='coerce').astype(float)\n",
    "    if 'ì²´ì¤‘(ëª¸ë¬´ê²Œ)' in df.columns:\n",
    "        df['ì²´ì¤‘(ëª¸ë¬´ê²Œ)'] = pd.to_numeric(df['ì²´ì¤‘(ëª¸ë¬´ê²Œ)'], errors='coerce').astype(float)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_meta = get_cleaned_meta(META_PATH)\n",
    "print(f\"Metadata loaded: {len(df_meta)} rows\")\n",
    "display(df_meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02f91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Train Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 1459.13it/s]\n",
      "Adding Test Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [00:00<00:00, 1496.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1088/1088 [384.5ms elapsed, 0s remaining, 2.8K samples/s]     \n",
      "Created dataset 'Hand Bone Image Segmentation' with 1088 samples.\n"
     ]
    }
   ],
   "source": [
    "if DATASET_NAME in fo.list_datasets():\n",
    "    fo.delete_dataset(DATASET_NAME)\n",
    "\n",
    "dataset = fo.Dataset(DATASET_NAME)\n",
    "samples = []\n",
    "\n",
    "# --- 1. Train ë°ì´í„° ì¶”ê°€ ---\n",
    "json_paths = glob.glob(os.path.join(TRAIN_LBL_ROOT, \"**/*.json\"), recursive=True)\n",
    "for j_path in tqdm(json_paths, desc=\"Adding Train Samples\"):\n",
    "    folder_name = os.path.basename(os.path.dirname(j_path))\n",
    "    f_id = int(''.join(filter(str.isdigit, folder_name)))\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ê²½ë¡œ ë§¤ì¹­ (outputs_json -> DCM)\n",
    "    img_path = j_path.replace('.json', '.png').replace('outputs_json', 'DCM')\n",
    "    if not os.path.exists(img_path): continue\n",
    "    \n",
    "    sample = fo.Sample(filepath=img_path, tags=[\"train\"])\n",
    "    sample[\"ID\"] = f_id\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ì£¼ì… (ì†ì„±ëª…: ID, ë‚˜ì´, ì„±ë³„, ì²´ì¤‘, í‚¤)\n",
    "    meta_row = df_meta[df_meta['ID'] == f_id]\n",
    "    if not meta_row.empty:\n",
    "        row = meta_row.iloc[0]\n",
    "        sample[\"Gender\"] = row['ì„±ë³„']\n",
    "        sample[\"Age\"] = row['ë‚˜ì´']\n",
    "        sample[\"Weight\"] = row['ì²´ì¤‘(ëª¸ë¬´ê²Œ)']\n",
    "        sample[\"Height\"] = row['í‚¤(ì‹ ì¥)']\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "# --- 2. Test ë°ì´í„° ì¶”ê°€ (ë¼ë²¨ì€ ì—†ì§€ë§Œ ë©”íƒ€ë°ì´í„°ëŠ” ì—°ê²°) ---\n",
    "test_pngs = glob.glob(os.path.join(TEST_IMG_ROOT, \"**/*.png\"), recursive=True)\n",
    "for t_path in tqdm(test_pngs, desc=\"Adding Test Samples\"):\n",
    "    folder_name = os.path.basename(os.path.dirname(t_path))\n",
    "    f_id = int(''.join(filter(str.isdigit, folder_name)))\n",
    "    \n",
    "    sample = fo.Sample(filepath=t_path, tags=[\"test\"])\n",
    "    sample[\"ID\"] = f_id\n",
    "    \n",
    "    meta_row = df_meta[df_meta['ID'] == f_id]\n",
    "    if not meta_row.empty:\n",
    "        row = meta_row.iloc[0]\n",
    "        sample[\"Gender\"] = row['ì„±ë³„']\n",
    "        sample[\"Age\"] = row['ë‚˜ì´']\n",
    "        sample[\"Weight\"] = row['ì²´ì¤‘(ëª¸ë¬´ê²Œ)']\n",
    "        sample[\"Height\"] = row['í‚¤(ì‹ ì¥)']\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "dataset.add_samples(samples)\n",
    "dataset.persistent = True\n",
    "print(f\"Created dataset '{DATASET_NAME}' with {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating GT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [04:23<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Complete!\n"
     ]
    }
   ],
   "source": [
    "with dataset.save_context() as context:\n",
    "    # 1. Train ë°ì´í„°ì˜ Ground Truth ì—…ë°ì´íŠ¸\n",
    "    for sample in tqdm(dataset.match_tags(\"train\"), desc=\"Updating GT\"):\n",
    "        json_path = sample.filepath.replace('.png', '.json').replace('DCM', 'outputs_json')\n",
    "        \n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, 'r') as f:\n",
    "                ann_data = json.load(f)\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ê°€ì ¸ì˜¤ê¸° (ì •ê·œí™”ìš©)\n",
    "            img = cv2.imread(sample.filepath)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            polylines = []\n",
    "            for ann in ann_data.get('annotations', []):\n",
    "                pts = ann.get('points', [])\n",
    "                norm_pts = [[(p[0]/w, p[1]/h) for p in pts]]\n",
    "                polylines.append(fo.Polyline(label=ann['label'], points=norm_pts, closed=True, filled=True))\n",
    "            \n",
    "            sample[\"ground_truth\"] = fo.Polylines(polylines=polylines)\n",
    "        \n",
    "        context.save(sample)\n",
    "\n",
    "print(\"Update Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17542853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded predictions: 8352 rows\n",
      "Adding predictions to dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 17/288 [00:03<00:57,  4.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [01:04<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prediction update complete!\n"
     ]
    }
   ],
   "source": [
    "# ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ ì£¼ì„í•´ì œ í›„ CSV_PATHì— csv ê²½ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import fiftyone as fo\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "CSV_PATH = \"\" # ğŸ ì‹œê°í™” ì›í•˜ëŠ” csv íŒŒì¼ ê²½ë¡œ ì…ë ¥\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(f\"âš ï¸ {CSV_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    pred_df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Loaded predictions: {len(pred_df)} rows\")\n",
    "\n",
    "    # 2. RLE ë””ì½”ë”© í•¨ìˆ˜ (ìˆ˜ì •ë¨: 'nan' ë¬¸ìì—´ ì²˜ë¦¬ ì¶”ê°€)\n",
    "    def decode_rle_to_mask(rle, height, width):\n",
    "        # rleê°€ ì‹¤ì œ NaNì´ê±°ë‚˜, ë¬¸ìì—´ \"nan\"ì´ë©´ ë¹ˆ ë§ˆìŠ¤í¬ ë°˜í™˜\n",
    "        if pd.isna(rle) or str(rle).lower() == 'nan': \n",
    "            return np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        s = str(rle).split() # ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ split\n",
    "        if not s: \n",
    "            return np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(height * width, dtype=np.uint8)\n",
    "        \n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        \n",
    "        return img.reshape(height, width)\n",
    "\n",
    "    # 3. ë§ˆìŠ¤í¬ -> FiftyOne Polyline ë³€í™˜ í•¨ìˆ˜\n",
    "    def mask_to_polylines(mask, label, img_w, img_h):\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        polylines = []\n",
    "        for contour in contours:\n",
    "            if len(contour) < 3: continue\n",
    "            points = contour.squeeze().astype(float)\n",
    "            if len(points.shape) < 2: continue\n",
    "            \n",
    "            # ì¢Œí‘œ ì •ê·œí™”\n",
    "            points[:, 0] /= img_w\n",
    "            points[:, 1] /= img_h\n",
    "            \n",
    "            polylines.append(fo.Polyline(\n",
    "                label=label,\n",
    "                points=[points.tolist()],\n",
    "                closed=True,\n",
    "                filled=True\n",
    "            ))\n",
    "        return polylines\n",
    "\n",
    "    # 4. ë°ì´í„°ì…‹ì— ì˜ˆì¸¡ê°’(Predictions) ì¶”ê°€í•˜ê¸°\n",
    "    sample_map = {os.path.basename(s.filepath): s for s in dataset.match_tags(\"test\")}\n",
    "    print(\"Adding predictions to dataset...\")\n",
    "    grouped = pred_df.groupby(\"image_name\")\n",
    "\n",
    "    with dataset.save_context() as context:\n",
    "        for image_name, group in tqdm(grouped, total=len(grouped)):\n",
    "            if image_name not in sample_map:\n",
    "                continue\n",
    "            \n",
    "            sample = sample_map[image_name]\n",
    "            all_polylines = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                rle = row['rle']\n",
    "                label = row['class']\n",
    "                \n",
    "                # ì—¬ê¸°ì„œ str(rle)ë¥¼ í•´ë„ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ 'nan' ì²´í¬ë¥¼ í•˜ë¯€ë¡œ ì•ˆì „í•¨\n",
    "                mask = decode_rle_to_mask(rle, 2048, 2048)\n",
    "                \n",
    "                # ë§ˆìŠ¤í¬ê°€ ë¹„ì–´ìˆìœ¼ë©´(0) polyline ë³€í™˜ ìŠ¤í‚µ\n",
    "                if mask.max() == 0:\n",
    "                    continue\n",
    "\n",
    "                polys = mask_to_polylines(mask, label, 2048, 2048)\n",
    "                all_polylines.extend(polys)\n",
    "            \n",
    "            if all_polylines:\n",
    "                sample[\"predictions\"] = fo.Polylines(polylines=all_polylines)\n",
    "                context.save(sample)\n",
    "\n",
    "    print(\"âœ… Prediction update complete!\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e88a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at localhost.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, port=5151, auto=False)\n",
    "'''\n",
    "ë¸Œë¼ìš°ì € ì°½ìœ¼ë¡œ ë³´ë ¤ë©´ VS Codeì—ì„œ í„°ë¯¸ë„ ì˜† Portsì— 5151 ì¶”ê°€ í›„ localhost ì ‘ì†í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
